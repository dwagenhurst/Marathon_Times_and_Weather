{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f70f4c7f-0d00-48e5-9ca5-32358a6bb4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import itertools\n",
    "import click\n",
    "import numpy as np\n",
    "\n",
    "#https://stackoverflow.com/questions/8049520/web-scraping-javascript-page-with-python\n",
    "from requests_html import HTMLSession, AsyncHTMLSession\n",
    "\n",
    "# https://www.youtube.com/watch?v=U6gbGk5WPws&list=PLzMcBGfZo4-n40rB1XaJ0ak1bemvlqumQ&index=3&ab_channel=TechWithTim\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import Select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eb1b9379-9574-4b6c-9ec4-e6cd05c504de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\AppData\\Local\\Temp/ipykernel_17052/1080277195.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    }
   ],
   "source": [
    "years = [str(i) for i in range(2001, 2019)] # excluding 2019, 2020, 2021 due to covid / virtual events\n",
    "base_url = 'http://registration.baa.org/cfm_Archive/iframe_ArchiveSearch.cfm?mode=entry&snap=60217012&'\n",
    "\n",
    "for year in years:\n",
    "    path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    driver = webdriver.Chrome(path)\n",
    "    driver.get(base_url)\n",
    "    soup = BeautifulSoup(driver.page_source)\n",
    "    \n",
    "# give a second to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "# select year from dropdown and search\n",
    "    select_year = Select(driver.find_element(by=By.NAME, value='RaceYearLowID')).select_by_visible_text(year)\n",
    "    search_button = driver.find_elements(by=By.CLASS_NAME, value='submit_button')\n",
    "    search_button[-1].click()\n",
    "    \n",
    "# grab page HTML\n",
    "    soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "# instantiate list for results\n",
    "    results = []         \n",
    "                                   \n",
    "# get number of runners (to know how many pages to go to)\n",
    "    runners = int(soup2.find_all('td')[2].text.split()[3])\n",
    "    \n",
    "# get results from current page and go on to next page\n",
    "    for i in range(runners // 25):\n",
    "    \n",
    "        table = soup2.find(\"table\", attrs={\"class\": \"tablegrid_table\"})\n",
    "        rows = table.findAll(\"tr\")\n",
    "        \n",
    "        for row in rows:\n",
    "            \n",
    "            a = [t.text.strip() for t in row.findAll(\"td\")][0:]\n",
    "#Don't store lines without data\n",
    "            if len(a) > 0 and a != [''] and a !=['',''] and a != ['', '', '']: \n",
    "                results.append(a)\n",
    "        \n",
    "                \n",
    "# Don't hammer the server. Give it a sec between requests.\n",
    "        time.sleep(.25)\n",
    "        \n",
    "# next page of results\n",
    "        next_button = driver.find_elements(by=By.CLASS_NAME, value='submit_button')\n",
    "        next_button[-1].click()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "# data is results but 'header rows' removed\n",
    "    data = []\n",
    "    for i, result in enumerate(results):\n",
    "        if i%3 == 0:\n",
    "            data.append(results[i] + results[i+1][1:])\n",
    "            \n",
    "# make data a df and save to csv    \n",
    "    columns = ['year', 'bib', 'name', 'age', 'gender', 'city', 'state', 'country', 'blank', \n",
    "               'overall_place', 'gender_place', 'division_place', 'official_time', 'net_time']\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    df.to_csv(f'./Boston_Results/Original/Boston_Results_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e519ade2-c0d4-456f-8c6c-b38445ccd414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
